{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize, word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.collocations import *\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_data(all_reviews):\n",
    "    full_text = ''\n",
    "    for i in range(len(all_reviews)):\n",
    "        full_text += all_reviews.iloc[i]['Review'] + '. ' \n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_data(full_text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    lemm = WordNetLemmatizer()\n",
    "\n",
    "    sentences = sent_tokenize(full_text)\n",
    "    clean_sentences = sent_tokenize(full_text)\n",
    "    for i in range(len(clean_sentences)):\n",
    "        clean_sentences[i] = lemm.lemmatize(clean_sentences[i])\n",
    "\n",
    "    tokens = tokenizer.tokenize(re.sub(\"[^-9A-Za-z ]\", \"\" , full_text))\n",
    "    tokens = [lemm.lemmatize(token.lower()) for token in tokens]\n",
    "\n",
    "\n",
    "    sr= stopwords.words('english')\n",
    "    clean_tokens = [token for token in tokens if token not in sr]\n",
    "\n",
    "    return clean_sentences, clean_tokens, sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacence_matrix(tokens):\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "    finder = BigramCollocationFinder.from_words(tokens)\n",
    "    finder.nbest(bigram_measures.pmi, 10)  \n",
    "\n",
    "    scored = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "    sor = sorted(bigram for bigram, score in scored)  \n",
    "\n",
    "    return sor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_step(word, sorted_ad_matrix):\n",
    "    ste_count = Counter(elem for elem in sorted_ad_matrix if elem[0] == word) + Counter(elem for elem in sorted_ad_matrix if elem[1] == word)\n",
    "    p = np.array(list(ste_count.values())) \n",
    "    p = p/sum(p)\n",
    "\n",
    "    temp = list(ste_count)[np.random.choice(len(ste_count), p=p)]\n",
    "    if temp[0] == word:\n",
    "        return temp[1]\n",
    "    return temp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_extractor(sorted_ad_matrix, n_words, phrases_per_word, in_words):\n",
    "    answer =[]\n",
    "    for in_word in in_words:\n",
    "        for j in range(phrases_per_word):\n",
    "            proto_phrase = ''\n",
    "            last_word = in_word\n",
    "            for i in range(n_words):\n",
    "                proto_phrase += last_word + ' '\n",
    "                last_word = next_step(last_word, sorted_ad_matrix)\n",
    "            answer.append(proto_phrase)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_mapper(phrases, sentences, original_sentences):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    lemm = WordNetLemmatizer()\n",
    "    best_sentences = []\n",
    "    for proto_phrase in phrases:\n",
    "        rank = []\n",
    "        for i in range(len(sentences)):\n",
    "            num_present = 0\n",
    "            sentence = sentences[i]\n",
    "            toki = tokenizer.tokenize(re.sub(\"[^-9A-Za-z ]\", \"\" , sentence))\n",
    "            toki = [lemm.lemmatize(toks.lower()) for toks in toki]\n",
    "            for proto_word in proto_phrase.split(' '):\n",
    "                if proto_word in toki:\n",
    "                    num_present +=1\n",
    "            rank.append(num_present)\n",
    "        best_sentences.append(original_sentences[rank.index(max(rank))])\n",
    "\n",
    "    #print(max(rank))\n",
    "    return best_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_extraction(full_text, n_words, phrases_per_word, in_words):\n",
    "    sentences, tokens, original_sentences = preprocessing_data(full_text)\n",
    "    sorted_ad_matrix = adjacence_matrix(tokens)\n",
    "    proto_phrases = path_extractor(sorted_ad_matrix, n_words, phrases_per_word, in_words)\n",
    "    selected_sentences = final_mapper(proto_phrases, sentences, original_sentences)\n",
    "    return selected_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'We went to the downtown SF location. The restaurant was really clean and servers were nice!Foods were great! We had a Burger and Ruben Sandwich! Delicious! We ordered a flight to taste 6 different beers! We enjoyed their brown ale and stout!We recommend this place for friends gathering!'"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "data = pd.read_json('data/factual_tripadvisor_restaurant_data_all_100_reviews.json')\n",
    "\n",
    "data['restaurants'].iloc[0]['reviews'][1]['review_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = ''\n",
    "for text in data['restaurants'].iloc[0]['reviews']:\n",
    "    full_text += text['review_text'] + '. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[\"Nothing spectacular but I'd go back.. Mediocre food (not bad, just mediocre, you can find the same food in pretty much every bar in the US, cooked just as well if not better).Service is so so and it's incredibly slow.\", 'Outdoor beer garden with benches outside on closed De BOOM street.']\n4.0\n"
     ]
    }
   ],
   "source": [
    "selected_sentences = full_extraction(full_text, 4, 2, ['food'])\n",
    "print(selected_sentences)\n",
    "print(data['restaurants'].iloc[0]['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nothing spectacular but I'd go back.. Mediocre food (not bad, just mediocre, you can find the same food in pretty much every bar in the US, cooked just as well if not better).Service is so so and it's incredibly slow.\n"
     ]
    }
   ],
   "source": [
    "print(selected_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}